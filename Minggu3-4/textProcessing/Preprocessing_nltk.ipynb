{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f24fe6a3-07eb-45a7-b428-e1c585a7758c",
   "metadata": {},
   "source": [
    "Preprocessing\n",
    "===\n",
    "***Alfin Rifaldi (A11.2022.14684)***\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2569ff37-cd06-4d3f-a9f0-c725a85e333a",
   "metadata": {},
   "source": [
    "# load library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e4eb715-5300-45f3-abfe-891385d16fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import time\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9018282-7670-4c06-a3b9-291b35a0f446",
   "metadata": {},
   "source": [
    "# load dataset\n",
    "\n",
    "Dataset ini menggunakan twitter data dan reddit data. \n",
    "Kali ini saya akan menggunakan twitter data terlebih dahulu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "509927be-bb5d-4482-9f94-129419b8dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb1d3c5b-b9d8-4ce6-8034-233ccd4d3b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data_Twitter.csv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a4811ef-8a24-450d-ad25-991d76948af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>User</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-03-31 14:32:04+00:00</td>\n",
       "      <td>pikobar_jabar</td>\n",
       "      <td>Ketahui informasi pembagian #PPKM di wilayah J...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-03-31 09:26:00+00:00</td>\n",
       "      <td>inewsdotid</td>\n",
       "      <td>Tempat Ibadah di Wilayah PPKM Level 1 Boleh Be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-03-31 05:02:34+00:00</td>\n",
       "      <td>vdvc_talk</td>\n",
       "      <td>Juru bicara Satgas Covid-19, Wiku Adisasmito m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-03-30 14:23:10+00:00</td>\n",
       "      <td>pikobar_jabar</td>\n",
       "      <td>Ketahui informasi pembagian #PPKM di wilayah J...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-03-30 11:28:57+00:00</td>\n",
       "      <td>tvOneNews</td>\n",
       "      <td>Kementerian Agama menerbitkan Surat Edaran Nom...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Date           User  \\\n",
       "0  2022-03-31 14:32:04+00:00  pikobar_jabar   \n",
       "1  2022-03-31 09:26:00+00:00     inewsdotid   \n",
       "2  2022-03-31 05:02:34+00:00      vdvc_talk   \n",
       "3  2022-03-30 14:23:10+00:00  pikobar_jabar   \n",
       "4  2022-03-30 11:28:57+00:00      tvOneNews   \n",
       "\n",
       "                                               Tweet  sentiment  \n",
       "0  Ketahui informasi pembagian #PPKM di wilayah J...          1  \n",
       "1  Tempat Ibadah di Wilayah PPKM Level 1 Boleh Be...          1  \n",
       "2  Juru bicara Satgas Covid-19, Wiku Adisasmito m...          1  \n",
       "3  Ketahui informasi pembagian #PPKM di wilayah J...          1  \n",
       "4  Kementerian Agama menerbitkan Surat Edaran Nom...          1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a051d4fb-4b72-45d8-9cdd-61e036bba7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "857309d6-88de-41c6-8ae5-8df64998affc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pikobar_jabar</td>\n",
       "      <td>Ketahui informasi pembagian #PPKM di wilayah J...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inewsdotid</td>\n",
       "      <td>Tempat Ibadah di Wilayah PPKM Level 1 Boleh Be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vdvc_talk</td>\n",
       "      <td>Juru bicara Satgas Covid-19, Wiku Adisasmito m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pikobar_jabar</td>\n",
       "      <td>Ketahui informasi pembagian #PPKM di wilayah J...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tvOneNews</td>\n",
       "      <td>Kementerian Agama menerbitkan Surat Edaran Nom...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23639</th>\n",
       "      <td>bananabluff</td>\n",
       "      <td>noelle loses a bet to akarsha and it somehow e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23640</th>\n",
       "      <td>Auqroix</td>\n",
       "      <td>they call her... weekeeshee...\\n#butterflysoup...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23641</th>\n",
       "      <td>Auqroix</td>\n",
       "      <td>put out what you wanna see more of, amirite ga...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23642</th>\n",
       "      <td>Auqroix</td>\n",
       "      <td>i don't need anybody, i'm fine here on my own\\...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23643</th>\n",
       "      <td>nindiiayudya</td>\n",
       "      <td>Selamat Siang sahabat online, nih aku sedikit ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23644 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                User                                              Tweet  \\\n",
       "0      pikobar_jabar  Ketahui informasi pembagian #PPKM di wilayah J...   \n",
       "1         inewsdotid  Tempat Ibadah di Wilayah PPKM Level 1 Boleh Be...   \n",
       "2          vdvc_talk  Juru bicara Satgas Covid-19, Wiku Adisasmito m...   \n",
       "3      pikobar_jabar  Ketahui informasi pembagian #PPKM di wilayah J...   \n",
       "4          tvOneNews  Kementerian Agama menerbitkan Surat Edaran Nom...   \n",
       "...              ...                                                ...   \n",
       "23639    bananabluff  noelle loses a bet to akarsha and it somehow e...   \n",
       "23640        Auqroix  they call her... weekeeshee...\\n#butterflysoup...   \n",
       "23641        Auqroix  put out what you wanna see more of, amirite ga...   \n",
       "23642        Auqroix  i don't need anybody, i'm fine here on my own\\...   \n",
       "23643   nindiiayudya  Selamat Siang sahabat online, nih aku sedikit ...   \n",
       "\n",
       "       sentiment  \n",
       "0              1  \n",
       "1              1  \n",
       "2              1  \n",
       "3              1  \n",
       "4              1  \n",
       "...          ...  \n",
       "23639          1  \n",
       "23640          1  \n",
       "23641          1  \n",
       "23642          1  \n",
       "23643          0  \n",
       "\n",
       "[23644 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7453e73-7347-4008-abd1-08539b60bc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23644 entries, 0 to 23643\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   User       23644 non-null  object\n",
      " 1   Tweet      23644 non-null  object\n",
      " 2   sentiment  23644 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 554.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4c3ecd-61d5-49fc-9e66-215d7e315674",
   "metadata": {},
   "source": [
    "# Preprocessing dataset\n",
    "saya akan pakai modul dari AMS 01-03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22b6758a-28c5-46c1-a378-5cace676bd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ekphrasis in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.5.4)\n",
      "Requirement already satisfied: termcolor in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ekphrasis) (2.4.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ekphrasis) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ekphrasis) (0.4.6)\n",
      "Requirement already satisfied: ujson in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ekphrasis) (5.9.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ekphrasis) (3.8.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ekphrasis) (3.8.1)\n",
      "Requirement already satisfied: ftfy in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ekphrasis) (6.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ekphrasis) (1.26.4)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ftfy->ekphrasis) (0.2.13)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->ekphrasis) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->ekphrasis) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->ekphrasis) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->ekphrasis) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->ekphrasis) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->ekphrasis) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->ekphrasis) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->ekphrasis) (2.9.0.post0)\n",
      "Requirement already satisfied: click in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk->ekphrasis) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk->ekphrasis) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk->ekphrasis) (2023.12.25)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\windows\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->ekphrasis) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ekphrasis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7de752-adc2-46f8-b13f-afd2d7757653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Windows\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ekphrasis\\classes\\tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word statistics files not found!\n",
      "Downloading... "
     ]
    }
   ],
   "source": [
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    #annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",'emphasis', 'censored'},\n",
    "    annotate={\"hashtag\"},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8871bd61-5ab0-4071-bc03-1d0c6ff2ac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# panggil ekphrasis\n",
    "\n",
    "def bersih_data(text):\n",
    "    return \" \".join(text_processor.pre_process_doc(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347a6e43-f39b-407c-a2b4-a9eb6db6d85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fungsi dari AMS 01-03. silakan cek bagaimana saya merubah menjadi fungsi\n",
    "\n",
    "def non_ascii(text):\n",
    "    return text.encode('ascii', 'replace').decode('ascii')\n",
    "\n",
    "def remove_space_alzami(text):\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "def remove_emoji_alzami(text):\n",
    "    return ' '.join(re.sub(\"([x#][A-Za-z0-9]+)\",\" \", text).split())\n",
    "\n",
    "def remove_tab(text):\n",
    "    return text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n",
    "\n",
    "def remove_tab2(text):\n",
    "    return re.sub('\\s+',' ',text)\n",
    "\n",
    "def remove_rt(text):\n",
    "    return text.replace('RT',\" \")\n",
    "\n",
    "def remove_mention(text):\n",
    "    return ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
    "\n",
    "def remove_incomplete_url(text):\n",
    "    return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "\n",
    "def remove_single_char(text):\n",
    "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "def remove_excessive_dot(text):\n",
    "    return text.replace('..',\" \")\n",
    "\n",
    "def change_stripe(text):\n",
    "    return text.replace('-',\" \")\n",
    "\n",
    "def lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_single_char(text):\n",
    "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "def remove_excessive_dot(text):\n",
    "    return text.replace('..',\" \")\n",
    "\n",
    "def lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_whitespace_LT(text):\n",
    "    return text.strip()\n",
    "\n",
    "def remove_whitespace_multiple(text):\n",
    "    return re.sub('\\s+',' ',text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    remove = string.punctuation\n",
    "    remove = remove.replace(\"_\", \"\") # don't remove hyphens\n",
    "    pattern = r\"[{}]\".format(remove) # create the pattern\n",
    "    return re.sub(pattern, \"\", text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bc94dd-5b1e-42de-bc05-32c36227af46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hapus untuk <>\n",
    "def remove_number_eks(text):\n",
    "    return text.replace('<number>',\" \")\n",
    "\n",
    "def remove_angka(text):\n",
    "    return re.sub(r\"\\d+\", \"\", text) \n",
    "\n",
    "def remove_URL_eks(text):\n",
    "    return text.replace('URL',\" \").replace('url',\" \")\n",
    "\n",
    "def space_punctuation(text):\n",
    "    return re.sub('(?<! )(?=[.,!?()])|(?<=[.,!?()])(?! )', r' ', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e328c863-7e97-4816-8863-7a2eded187d4",
   "metadata": {},
   "source": [
    "## lakukan pembersihan dengan memanggil fungsi yang didefinisikan diatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90fddcd-fbbc-46af-aeeb-64c2658dd2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "final_string = []\n",
    "s = \"\"\n",
    "for text in df['Tweet'].values:\n",
    "    filteredSentence = []\n",
    "    EachReviewText = \"\"\n",
    "    proc = remove_rt(text)\n",
    "    proc = lower(proc)\n",
    "    proc = change_stripe(proc)\n",
    "    proc = remove_emoji_alzami(proc)\n",
    "    proc = remove_tab(proc)\n",
    "    proc = remove_tab2(proc)\n",
    "    proc = non_ascii(proc)\n",
    "    proc = remove_incomplete_url(proc)\n",
    "    proc = remove_excessive_dot(proc)\n",
    "    proc = remove_whitespace_LT(proc)\n",
    "    proc = remove_whitespace_multiple(proc)\n",
    "    proc = remove_single_char(proc)\n",
    "    proc = space_punctuation(proc)\n",
    "    proc = remove_punctuation(proc)\n",
    "    proc = remove_space_alzami(proc)\n",
    "    proc = bersih_data(proc)\n",
    "    proc = remove_number_eks(proc)\n",
    "    proc = remove_angka(proc) \n",
    "    proc = remove_URL_eks(proc)\n",
    "    EachReviewText = proc\n",
    "    final_string.append(EachReviewText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5359042e-a6af-4225-8b07-a67a04bf863c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"step01\"] = final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b4c87-f47d-4bde-a648-3ea018ae5b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b7e590-0b95-40aa-bd9c-dd98af113086",
   "metadata": {},
   "source": [
    "## hapus data kosong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5d187b-2309-4724-beae-cade238077b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a524b26-dcee-49f0-bef1-9047fe30a59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hapus = df[~df['step01'].str.contains(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5fc7dc-fad5-459e-ac5f-716fa7ccb49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hapus.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f7f3ec-47cd-4e83-a765-a61f60d11dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hapus.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7ce1b3-f407-4d4b-afe7-792097222535",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df[~df.isin(df_hapus)].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ae3049-3adc-4eab-8bf4-9ba01253244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e882a5e6-1781-47e8-8b98-d30470647620",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cbfc1f-4ca4-487c-90a3-b4fb8808cdb0",
   "metadata": {},
   "source": [
    "## normalisasi kata slang menjadi kata baku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c06d14-8cac-462d-8d64-5799f1e0c8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88fe959-f3bb-425d-9e00-7be21b3941af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize_wrapper(text):\n",
    "  return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81df808a-8754-4bc5-b3bc-a32b7185ed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['tokens'] = df['step01'].apply(word_tokenize_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c955ca-2646-4915-9ffd-d6633fef906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11342c7-ff6f-4ed3-aa10-046012774d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_word = pd.read_csv('kamus_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe1c817-41ab-469d-bccb-c51971d43eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_word_dict = {}\n",
    "\n",
    "for index, row in normalized_word.iterrows():\n",
    "    if row[0] not in normalized_word_dict:\n",
    "        normalized_word_dict[row[0]] = row[1] \n",
    "\n",
    "def normalized_term(document):\n",
    "    return [normalized_word_dict[term] if term in normalized_word_dict else term for term in document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a934fb-d26e-42bc-a36c-b28b4c70108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['final_tokens'] = df_new['tokens'].apply(normalized_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec795b71-06c5-4da3-bdac-abc56fa25502",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "final_string_tokens = []\n",
    "for text in df_new['final_tokens'].values:\n",
    "    EachReviewText = \"\"\n",
    "    EachReviewText = ' '.join(text)\n",
    "    final_string_tokens.append(EachReviewText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba16f44-7c09-4904-b38e-fc0e27e0cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[\"step02\"] = final_string_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c0ab53-0891-474a-a013-0269abef506a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a045c0-78d6-46a5-9cb8-5dd0c2d683b5",
   "metadata": {},
   "source": [
    "# simpan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a530661-0c79-45a4-9d6d-ba57f2d02208",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv('clean_dataset.csv',sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb279d4-b860-40c7-ac1f-88a7d3ee8fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
